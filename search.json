[
  {
    "objectID": "page-research.html",
    "href": "page-research.html",
    "title": "Research",
    "section": "",
    "text": "‘Policy Paper’ \nCost Evaluation in Education \nHealthcare and Costs\nSummary of Clausing\nLabor Market Update - November 2023"
  },
  {
    "objectID": "page-index.html",
    "href": "page-index.html",
    "title": "Tyler Merrill",
    "section": "",
    "text": "Welcome!\nThis domain serves as a repository of my work, including samples from my academic and professional portfolios. Please feel free to browse, or download any files for personal use.\n\nNotice: This is a grad student’s personal project silo. As such, it comes with no guarantees of any descriptor."
  },
  {
    "objectID": "page-assignment1.html",
    "href": "page-assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Statistics, when considered as a branch of reasoning, is occasionally harangued in the everyday world as being a worse base for forming conclusions than proverbial ‘lies, and damned lies’. Of course, the undertone of the sentiment is often to simply convey a gentle reminder about the dishonest (or at least incorrect) use of statistical tools to imply associations that are unsupportable on a sounder basis of application. A dichotomous lesson many first receive in an introductory statistics course while pursuing studies at an institution of higher education.\nOf course, the argument over the soundest basis for the use of statistical tools has a more rarefied pedigree within academia itself. Especially as regards their application in the social sciences. In this data-driven century, which comfortably weaves patterns of information out of the historically unparalleled petabyte range (Daoud and Dubhashi 2023, 27), what can best be conceptualized as schools of thought have emerged with competing understandings of the proper foundations and appropriate spheres of application of statistical methods (Daoud and Dubhashi, 2023). Two prominent instalments of this debate will be surveyed here. Namely, Leo Breiman’s 2001 article Statistical Modeling: The Two Cultures and Galit Shmueli’s 2010 paper To Explain or to Predict?.\nThe issue, as per usual, starts at the beginning: What is the purpose of doing statistics? The answer, according to Breiman, is to contribute to the advancement of science by allowing us to make predictions (Breiman 2001, 11–12). He mentions how in the private-sector, the only value of statistical analysis rests on its predictive accuracy (2001, 3-4). In other words, a statistic’s worth lies in the results it actuates. He also observes that, in contrast, academics involved in the pursuit of scientific knowledge typically value statistical tools for their ability to provide an interpretable, causal model of some phenomena under study (Breiman 2001, 4–5).1 Perhaps even ignoring matters of predictive strength (or lack thereof) in favour of “the construction of an ingenious stochastic model.” (2001, 5).\nNow, the epistemological underpinnings that Breiman is alluding to come clearly into the fore in his construal of why different practitioners value different criteria to adjudicate the ends of the scientific enterprise. Breiman sees the differing teleological visions of the two ‘camps’ of practitioners to be essentially conceptual instantiations of the properties of the tools they use to advance each other’s conception of knowledge (Breiman 2001, 1). Traditional statistical analysis often involves the use of linear parametric2 models that are presumed to represent the phenomena that are the origin of the data that is available to study (Breiman 2001, 1, 4–5, 6). These models are often chosen independently of the data under consideration and then tested against various goodness-of-fit measures (2001, 4). Breiman expresses credulity that anyone could seriously believe that such limited models, supposedly chosen because of their ease of manipulation and understanding (Breiman 2001, 4–6), would necessarily describe much of the complicated nature of reality (2001, 4-5). Here, he attacks the classical presumption of traditional modelling on its own terms. In essence, arguing that even if classical modelling’s focus on finding the ‘real’ nature of data-generating processes (Breiman 2001, 1) were the most fruitful goal of scientific inquiry, simple parametric models are not going to get anyone very far.\nIndeed, Breiman believes that classical research has run up against the limits of such methodologies3 due to the preponderance of larger and more complicated data sets that have become available since the digital age (Breiman 2001, 6, 8). Responding to the (traditional statisticians) hammer and (parametrically subsumable) nail analogy, he notes: “The trouble for statisticians is that recently some of the problems have stopped looking like nails.” (2001, 6). He vouches for a range of modern, computationally powerful tools as providing a path forward that he subsumes under the nomenclature of the “Algorithmic Modeling Culture”, AMC, as opposed to consigning traditional academics to a “Data Modeling Culture” or DMC (2001, 1).\nPoignantly, Breiman holds that the best models are those that provide the highest predictive accuracy, such as neural nets, random trees (or ‘forests’ composed of them), and support vector algorithms (2001, 9-11). He notes that despite their impenetrability to easy (or any) understanding, such ‘black box’ processes are to be preferred to more traditional (parametric) ones because “The goal is not interpretability, but accurate information.” (Breiman 2001, 12). To bring the point home, in response to traditional objections to using such complicated algorithmic tools because they do not lend themselves to interpretability, he notes the “…wrong question is being asked.” (2001, 11)\nIn contradistinction, Galit Shmueli focuses not on the merits of DMC or AMC cultures per se but more on the rationale for the different enterprises of predicting new data and explaining currently existing data sets and the resulting impact of an asymmetrical focus on their distinction (Shmueli 2010, 1, 16). Shmueli organizes his thesis around an axis of bifurcated aspects of modes of ‘doing statistical analysis’: ‘Causation–Association’, ‘Theory–Data’, ‘Retrospective–Prospective’, and ‘Bias–Variance’ (2010, 5). Explanatory modelling and predicative analysis4 are conceptually situated opposite to each other in every pair of this scheme, in order to reveal their fundamental differences of modality (2010, 5).\nFor starters, explanatory modelling is supposed to specify all the relevant variables that are assumed to cause an outcome(s) of interest, as well as undergird the causal pathways as of to how the variables cause those outcome(s) (Shmueli 2010, 2). He notes that in many disciplines, mere association-based tools5 such as regression analysis are used on data for “testing causal hypotheses about theoretical constructs” (2010, 2-3). Meaning that ‘extra-modular’ inferences must buttress the theory.6 Therefore, following his axis paradigm in explanatory modeling (2010, 5):\n1) Causation channels are supposed to be captured by some sort of statistical model from which to draw conclusions of ontological significance (an act of inference)\n2) This model is often constructed with theoretical considerations in mind\n3) The focus is on data that already exists and\n4) It focuses on minimizing bias in order to properly undercover the magnitude of various variables.\nThe intrinsic value of such an approach is, therefore, assumed to lie in its ability to cohere data into a story the human mind can appreciate. As theoretical physicist David Deutsch remarks in the conclusion to the quote that opened this article:\nPredictive modelling, on the other hand, via the same axis is (Shmueli 2010, 5, 11):\n1) Concerned primarily with associations, whatever their real underlying relationship\n2) Estimates a model drawn from the data and not independently from it\n3) Is focused on future, currently non-existent data and\n4) Is willing to incur bias if need be in order to improve estimation.\nIn concurrence with Breiman, Shmueli mentions that predictive modelling is able to take advantage of a “…range of plausible methods [that]7 includes not only statistical models (interpretable and uninterpretable) but also data mining algorithms.” (2010, 10). Even if those algorithms “… are considered ill-suited for explanatory modeling.” (2010, 10).\nWhere Shmueli shines a spotlight, however, concerns the fact that an explicit recognition of the distinction in conditions required for the different types of analysis has not always been appreciated in the literature (Shmueli 2010, 6, 16). A state of affairs that has led to an arguable loss to scientific progress, not only due to matters of epistemological rectitude but because the contributions that algorithmic/predictive modelling could make to explanatory modelling has gone underappreciated (2010, 16-18). Shmueli pleads that “by producing high-accuracy price predictions they shed light on new potential variables that are related to…the types of relationships that can be further investigated in terms of causality.” (2010, 16). In contrast to Breiman’s championing of AMC over DMC practices (Breiman 2001, 16), Shmueli sees a role for the ‘two cultures’ to work together - although, to reiterate, he is also convinced that traditional statistical practice has harmed itself by not more proactively embracing the new tools provided by predictive algorithms (Shmueli 2010, 16).\nIn the conciliatory respect, Shmueli has company with Adel Daoud and Devdatt Dubhashi, whose article Statistical Modeling: The Three Cultures acts as a partial rejoinder to Breiman’s and advocates a path not dissimilar to that Shmueli does. They argue that Breiman does not provide an argument as of to why predication advances scientific progress (Daoud and Dubhashi 2023, 30) and aim to provide their own counterargument that ends up aligning with some of the aforementioned positions staked out by Shmueli (Shmueli 2010, 18–22, 24–27, 32). Daoud and Dubhashi argue that rather than best considered as antagonists to explanatory frameworks at the other end of a hostile spectrum,8 that the methods of algorithmic learning models have developed a synergetic coupling with explanatory modelling among some researchers (2023, 17). This has led to the formation of what they term a hybrid modelling culture, or HMC (2023, 3, 5, 17).\nThey reason this culture has flourished because it can offer a new alley-way for traditional causal frameworks9 to expand the range of potential pathways that can be brought into their service via machine-learning/prediction algorithms (2023, 5, 33-34). While they point out that isolated approaches of either vein are useful for certain problems (2023, 16, 27, 32), and HMC type approaches come with their own potential problems (34), they believe that the most lucrative direction for the applied social sciences lies in utilizing the strength of algorithmic modelling to better complement the goal usually left to more traditional statistical inference (35).\nTraditional inference concerns such as endogeneity (Shmueli 2010, 9–10) and the struggle to ‘operationalize’ estimators of population parameters (2010, 2, 14-15, 19) will likely still occupy researchers time even if adoption of learning-based algorithms becomes widespread. However, it seems fair to concede that problems related to systemic errors (Daoud and Dubhashi 2023, 27–18) on the scales only possible to encounter with ever larger data sets will probably be better solved by aiding the ‘mind’s eye’ with a large number of relatively blind but perhaps further seeing algorithmic ones.\nLooking towards an ever more data-rich world and very a human desire to understand it, let us hope the words of one researcher are heeded:"
  },
  {
    "objectID": "page-assignment1.html#footnotes",
    "href": "page-assignment1.html#footnotes",
    "title": "Assignment 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs well as because truly verifiable, measurable, randomized introductions of exogenous variables (experiments) are hard to come by ‘in the wild’, especially in the social sciences (see footnote 6), and causal perturbation effects need to be identified by accounting (‘controlling’) for possible missing/hidden variables (Huntington-Klein 2022, ch. 4, 8-11, passim). A task that can therefore be aided considerably if it can be assumed the underlying relationships are described by a well-defined parametric model.↩︎\nEssentially, a model whose structure is presumed in advance, hence making for a more simplified grasp of reality (Hoskin n.d.).↩︎\nThese limits are arguably always there anyway, in a sense that has been recognized for centuries: “But philosophers, who carry their scrutiny a little farther, immediately perceive, that, even in the most familiar events, the energy of the cause is as unintelligible as in the most unusual, and that we only learn by experience the frequent CONJUNCTION of objects, without being ever able to comprehend any thing like CONNEXION between them.” - (Hume et al. 1993, 52)↩︎\nHe also focuses on purely descriptive analysis, see (2010, 3).↩︎\n“Whereas “proper” statistical methodology for testing causality exists, such as designed experiments or specialized causal inference methods for observational data…” (2010, 3).↩︎\n“In complex situations—and most social science is about highly complex situations—we may well not know very much about what the causal…looks like.” - (Huntington-Klein 2022, 161)↩︎\nItalics mine.↩︎\nIt is important to note that Shmueli also sees the antagonism as fairly historic in nature and views the two approaches as more of a set of potentially complimentary dimensions (2010, 17).↩︎\nThey take the ‘hypothetico-deductive’ method as the central one (Daoud and Dubhashi 2023, 5)↩︎"
  },
  {
    "objectID": "page-about.html",
    "href": "page-about.html",
    "title": "About",
    "section": "",
    "text": "I am a grad student at the University of Texas at Dallas. I am honored to be apart of the diverse and dedicated student body of such a pivotal institution. With some of Texas’s foremost instructors, I am continuously learning the skills needed to tackle the challenges facing dynamic organizations across the globe. I hope to use these assets to assist businesses and governments wherever they are with the ever changing needs of growth and development.\n\nI also have nearly a decade of experience working in the customer service industry. A rewarding career that has taught me the invaluableness of teamwork, and what can be accomplished when people work together. I have performed roles utilizing skills as varied as logistical coordination, management, and shipping accommodation. I am also a firm believer in the utility brought about by an inclusive workforce. The meshing of differing approaches to any task at hand is any institution’s ultimate resource.\n\nI am excited to continue expanding my knowledge base and skill set in order to achieve my professional goals. I look forward to the journey I will share with my fellow classmates in the near future and my co-workers in the longer one."
  },
  {
    "objectID": "page-assignment2.html",
    "href": "page-assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "To begin this assignment, I loaded the requisite data set, ‘TED2016’ into R studio (after initiating the ‘tidyverse’ and ‘haven’ packages) with the following code:   \nread_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nNaming the dataset as ‘TEDS_2016’. Of course one of the first things to notice is the amount of variables this dataset contains. 54 in total! Arguably far too many to immediately figure out how to parse.\nTo get a preliminary ‘hold’ off the data, I go ahead and clean the variable name using:\nTEDS_2016 |&gt; janitor::clean_names() |&gt; mutate(Tondu = factor(Tondu))\nI then take a look at the entire variable (column) list. I notice immediately that some of the 1,690 observations are missing, ‘NA’. I figure my first step should be to deal with these before proceeding too far along. But also, some of the variables appear to be categorical/binary. A fact I can at least cursorily confirm by opening the visualization extension in RStudio’s ‘Environment’ pane.\nI also note that some of the columns might be combing information from other variables. It appears, after taking another glance at the variables, that all the predictors are of a ‘dbl’ or dbl+lbl’ type.\n\n\n\n\nA missing value can be alikened to a piece of reality that refuses to show up. You figure it must be there, so one is tempted to ignore or rationalize for it, yet it may get in the way of being able to deal with what is at least available. But, for the moment I used the following dplyr command to remove all rows with missing values:\nTEDS_2016[complete.cases(TEDS_2016), ]\nThis poses a serious risk of introducing basis in my analysis from this point forward. Indeed, this move reduced the number of rows by 626! So in the interests of data integrity, I decide to proceed with the missing values.\n\n\n\n\nIn order to compare the variable ‘Tondu’ with the variables ‘female’, ‘DPP’, ‘age’, ‘income’, ‘edu’, ‘Taiwanese’ and ‘Econ_worse’, I utilize the ‘plot’ command to see how R sorts these types of variables. Specifically for ‘educ’ and ‘income’, I try:\nggplot(TEDS_2016, aes(x = income, y = Tondu)) +\n  geom_bar(stat='identity')\n…and so on.\n\n\n\nAgainst ‘educ’\n\n\n\n\n\nAgainst ‘income’\n\n\nAs these are categorical variables, these bar-plots appear to show the relative mixes of the various tiers of the selected variables. For ‘votetsai’, ‘female’, and ‘DPP’, I will attempt to compare binary variables with a categorical one. I attempt to estimate the average effect on an additional ‘climb’ up the ‘Tondu’ scale by running a series of linear regressions like so:\n{attach(TEDS_2016)} Tondu=as.factor(Tondu) lm.fit=lm(female~Tondu,data=TEDS_2016)\nWhich produces:\n\n\n\nAgainst ‘female’\n\n\n\n\n\nAgainst ‘votetaisai’\n\n\n\n\n\nAgainst ‘DPP’\n\n\nI can interpret these as telling me that for every step up the ‘Tondu’ scale, its becomes X(=Tondu)*(beta) more likely for the indicator variable to edge towards a value of ‘1’.\n\n\n\n\n\n\nOf course at this stage, we are just playing with the data. We do not yet know for sure whether or not our classifications are appropriate.\n\n\n\n\n\n\nBut let’s go ahead and wrap up with a histogram and frequency polygon snapshot of the ‘Tondu’ variable:\n\n\n\nHistogram\n\n\n\n\n\nFrequency Polygon\n\n\nWhich roughly shows us the concentration of this variable."
  },
  {
    "objectID": "page-assignment2.html#initialization",
    "href": "page-assignment2.html#initialization",
    "title": "Assignment 2",
    "section": "",
    "text": "To begin this assignment, I loaded the requisite data set, ‘TED2016’ into R studio (after initiating the ‘tidyverse’ and ‘haven’ packages) with the following code:   \nread_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nNaming the dataset as ‘TEDS_2016’. Of course one of the first things to notice is the amount of variables this dataset contains. 54 in total! Arguably far too many to immediately figure out how to parse.\nTo get a preliminary ‘hold’ off the data, I go ahead and clean the variable name using:\nTEDS_2016 |&gt; janitor::clean_names() |&gt; mutate(Tondu = factor(Tondu))\nI then take a look at the entire variable (column) list. I notice immediately that some of the 1,690 observations are missing, ‘NA’. I figure my first step should be to deal with these before proceeding too far along. But also, some of the variables appear to be categorical/binary. A fact I can at least cursorily confirm by opening the visualization extension in RStudio’s ‘Environment’ pane.\nI also note that some of the columns might be combing information from other variables. It appears, after taking another glance at the variables, that all the predictors are of a ‘dbl’ or dbl+lbl’ type."
  },
  {
    "objectID": "page-assignment2.html#missing-truths",
    "href": "page-assignment2.html#missing-truths",
    "title": "Assignment 2",
    "section": "",
    "text": "A missing value can be alikened to a piece of reality that refuses to show up. You figure it must be there, so one is tempted to ignore or rationalize for it, yet it may get in the way of being able to deal with what is at least available. But, for the moment I used the following dplyr command to remove all rows with missing values:\nTEDS_2016[complete.cases(TEDS_2016), ]\nThis poses a serious risk of introducing basis in my analysis from this point forward. Indeed, this move reduced the number of rows by 626! So in the interests of data integrity, I decide to proceed with the missing values."
  },
  {
    "objectID": "page-assignment2.html#comparing-variables-with-tondu",
    "href": "page-assignment2.html#comparing-variables-with-tondu",
    "title": "Assignment 2",
    "section": "",
    "text": "In order to compare the variable ‘Tondu’ with the variables ‘female’, ‘DPP’, ‘age’, ‘income’, ‘edu’, ‘Taiwanese’ and ‘Econ_worse’, I utilize the ‘plot’ command to see how R sorts these types of variables. Specifically for ‘educ’ and ‘income’, I try:\nggplot(TEDS_2016, aes(x = income, y = Tondu)) +\n  geom_bar(stat='identity')\n…and so on.\n\n\n\nAgainst ‘educ’\n\n\n\n\n\nAgainst ‘income’\n\n\nAs these are categorical variables, these bar-plots appear to show the relative mixes of the various tiers of the selected variables. For ‘votetsai’, ‘female’, and ‘DPP’, I will attempt to compare binary variables with a categorical one. I attempt to estimate the average effect on an additional ‘climb’ up the ‘Tondu’ scale by running a series of linear regressions like so:\n{attach(TEDS_2016)} Tondu=as.factor(Tondu) lm.fit=lm(female~Tondu,data=TEDS_2016)\nWhich produces:\n\n\n\nAgainst ‘female’\n\n\n\n\n\nAgainst ‘votetaisai’\n\n\n\n\n\nAgainst ‘DPP’\n\n\nI can interpret these as telling me that for every step up the ‘Tondu’ scale, its becomes X(=Tondu)*(beta) more likely for the indicator variable to edge towards a value of ‘1’.\n\n\n\n\n\n\nOf course at this stage, we are just playing with the data. We do not yet know for sure whether or not our classifications are appropriate.\n\n\n\n\n\n\nBut let’s go ahead and wrap up with a histogram and frequency polygon snapshot of the ‘Tondu’ variable:\n\n\n\nHistogram\n\n\n\n\n\nFrequency Polygon\n\n\nWhich roughly shows us the concentration of this variable."
  },
  {
    "objectID": "page-project.html",
    "href": "page-project.html",
    "title": "Project",
    "section": "",
    "text": "Project docs to go here. Check back soon!"
  }
]